# -*- coding: utf-8 -*-
"""worldsmood.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iiCZb-YMqgpH9TlFWVWNQmNDv6hcYZyx

# worlds words
"""

"""
  Global News Word Analysis - MVP v6 (RSS Feeds)
  Collects 50+ news articles per country from major news outlets via RSS
  and finds the most prevalent words that are locally common but globally rare.
  """

  # Step 1: Install required packages
# !pip install feedparser pandas requests beautifulsoup4 nltk -q

# Step 2: Import libraries
import feedparser
import pandas as pd
from datetime import datetime
import time
from collections import Counter
import requests
from bs4 import BeautifulSoup
import signal # Import signal for timeout
from contextlib import contextmanager # Import contextmanager for timeout
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download required NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    print("NLTK download failed, using basic tokenization")

# Parameter to control the number of countries to process
MAX_COUNTRIES = None # Process all countries (no limit)


# Step 3: Configuration - Major news outlets by country
# Expanded list to include a wider range of countries for better map coverage
NEWS_SOURCES = {
      'Canada': [
          'https://torontosun.com/feed/', # Alternative Canadian source 1
          'https://montrealgazette.com/feed/', # Alternative Canadian source 2
          'https://vancouversun.com/feed/' # Alternative Canadian source 3
      ],
      'United States': [
          'http://rss.cnn.com/rss/cnn_topstories.rss',
          'https://feeds.nbcnews.com/nbcnews/public/news',
          'http://feeds.foxnews.com/foxnews/latest',
      ],
      'United Kingdom': [
          'http://feeds.bbci.co.uk/news/rss.xml',
          'https://www.theguardian.com/uk/rss',
          'https://www.independent.co.uk/news/uk/rss',
      ],
      'Germany': [
          'https://www.dw.com/en/top-stories/s-9097/rss',
          'https://www.spiegel.de/international/index.rss',
      ],
      'France': [
          'https://www.france24.com/en/rss',
          'http://www.rfi.fr/en/rss',
      ],
      'Japan': [
          'https://www3.nhk.or.jp/nhkworld/en/news/rss.xml',
          'https://www.japantimes.co.jp/feed/',
      ],
      'Australia': [
          'https://www.abc.net.au/news/feed/2942460/rss.xml',
          'https://www.smh.com.au/rss/feed.xml',
      ],
      'Brazil': [
          'https://rss.uol.com.br/feed/noticias.xml',
      ],
      'India': [
          'https://timesofindia.indiatimes.com/rssfeedstopstories.cms',
          'https://www.hindustantimes.com/rss/topnews/rssfeed.xml',
      ],
      'South Korea': [
          'https://world.kbs.co.kr/rss/rss_news.htm?lang=e',
      ],
      'Italy': [
          'https://www.ansa.it/english/news/general_news.rss',
      ],
      'Spain': [
          'https://elpais.com/rss/elpais/portada.xml',
      ],
      'Mexico': [
          'https://www.eluniversal.com.mx/rss.xml',
      ],
      'Russia': [
          'https://tass.com/rss/v2.xml',
      ],
      'China': [
          'http://www.chinadaily.com.cn/rss/china_rss.xml',
      ],

      'Argentina': [
          'https://www.batimes.com.ar/feed/rss',
      ],
      'Egypt': [
          'https://english.ahram.org.eg/RSS/Egypt.aspx',
      ],
      'Nigeria': [
          'https://www.premiumtimesng.com/feed',
      ],
      'South Africa': [
          'https://www.news24.com/news24/SouthAfrica/rss',
      ],
      'Indonesia': [
          'https://www.thejakartapost.com/rss/headline',
      ],
      'Turkey': [
          'https://www.hurriyetdailynews.com/rss/feed',
      ],
      'Saudi Arabia': [
          'https://english.alarabiya.net/.sections/___/rss.xml',
      ],
      'Iran': [
          'https://www.tehrantimes.com/rss',
      ],
      'Pakistan': [
          'https://www.dawn.com/rss/all',
      ],
      'Bangladesh': [
          'https://www.thedailystar.net/ارشيف/rss.xml', # This URL might need verification/correction
      ],
      'Philippines': [
          'https://www.philstar.com/rss/headlines',
      ],
      'Thailand': [
          'https://www.bangkokpost.com/rss/news',
      ],
      'Vietnam': [
          'https://vietnamnews.vn/rss/latest-news.rss',
      ],
      'Poland': [
          'https://www.thefirstnews.com/feed',
      ],
      'Ukraine': [
          'https://www.kyivpost.com/feed',
      ],
      'Sweden': [
          'https://www.thelocal.se/feeds/rss.php',
      ],
      'Norway': [
          'https://www.thelocal.no/feeds/rss.php',
      ],
       'Denmark': [
          'https://www.thelocal.dk/feeds/rss.php',
      ],
      'Finland': [
          'https://www.helsinkitimes.fi/feed',
      ],
      'Greece': [
          'https://www.ekathimerini.com/rss',
      ],
      'Portugal': [
          'https://www.theportugalnews.com/rss',
      ],
      'Ireland': [
          'https://www.rte.ie/news/rss/',
      ],
      'Belgium': [
          'https://www.brusselstimes.com/feed',
      ],
      'Netherlands': [
          'https://www.dutchnews.nl/feed/',
      ],
      'Switzerland': [
          'https://www.swissinfo.ch/eng/rss/index.xml',
      ],
      'Austria': [
          'https://www.thelocal.at/feeds/rss.php',
      ],
      'Czech Republic': [
          'https://www.expats.cz/rss/news',
      ],
      'Hungary': [
          'https://dailynewshungary.com/feed/',
      ],
      'Romania': [
          'https://www.romania-insider.com/feed',
      ],
      'Bulgaria': [
          'https://sofiaglobe.com/feed/',
      ],
      'Serbia': [
          'https://rs.n1info.com/feed/', # This URL might need verification/correction
      ],
      'Croatia': [
          'https://www.total-croatia-news.com/feed',
      ],
      'Slovenia': [
          'https://www.total-slovenia-news.com/feed',
      ],
      'Slovakia': [
          'https://spectator.sme.sk/rss/',
      ],
      'Lithuania': [
          'https://www.baltictimes.com/rss/feed.xml',
      ],
      'Latvia': [
          'https://eng.lsm.lv/rss/',
      ],
      'Estonia': [
          'https://news.err.ee/rss',
      ],
      'New Zealand': [
          'https://www.nzherald.co.nz/rss/headlines.cfm',
      ],
      'Singapore': [
          'https://www.straitstimes.com/rss.xml',
      ],
      'Malaysia': [
          'https://www.thestar.com.my/rss/latest-news',
      ],
      'Chile': [
          'https://www.santiagotimes.cl/feed/',
      ],
      'Colombia': [
          'https://www.colombiareports.com/feed/',
      ],
      'Peru': [
          'https://andina.pe/ingles/rss.aspx',
      ],
      'Venezuela': [
          'https://en.ultimasnoticias.com.ve/feed/', # This URL might need verification/correction
      ],
      'Ecuador': [
          'https://www.ecuadortimes.net/feed/',
      ],
      'Bolivia': [
          'https://boliviatoday.com/feed/',
      ],
      'Paraguay': [
          'https://www.ultimahora.com/rss/portada.xml', # This URL might need verification/correction
      ],
      'Uruguay': [
          'https://mercopress.com/rss/headlines',
      ],
      'Algeria': [
          'https://www.aps.dz/en/feed',
      ],
      'Morocco': [
          'https://www.moroccoworldnews.com/feed',
      ],
      'Tunisia': [
          'https://www.tunisiatimes.com/feed/', # This URL might need verification/correction
      ],
      'Libya': [
          'https://www.libyaobserver.ly/rss',
      ],
      'Sudan': [
          'https://sudantribune.com/feed/',
      ],
      'Ethiopia': [
          'https://addisstandard.com/feed/',
      ],
      'Kenya': [
          'https://www.standardmedia.co.ke/rss/headlines.xml',
      ],
      'Uganda': [
          'https://www.monitor.co.ug/uganda/rss',
      ],
      'Ghana': [
          'https://www.graphic.com.gh/news/general-news.rss',
      ],
      'Ivory Coast': [
          'https://news.abidjan.net/h/rss/news.xml', # This URL might need verification/correction
      ],
      'Cameroon': [
          'https://www.journalducameroun.com/en/feed/',
      ],
      'Senegal': [
          'https://www.jeuneafrique.com/rss.xml', # This is a general African news source, might not be specific to Senegal
      ],
      'Democratic Republic of Congo': [
          'https://www.radiookapi.net/en/rss', # Radio Okapi, might not be comprehensive news
      ],
      'Angola': [
          'https://www.angop.ao/angola/en_us/rss.xml',
      ],
      'Mozambique': [
          'https://clubofmozambique.com/feed/',
      ],
      'Madagascar': [
          'https://www.madagascar-tribune.com/spip.php?page=backend&lang=en', # This URL might need verification/correction
      ],
      'Yemen': [
          'https://www.almasdaronline.com/rss/en',
      ],
      'Syria': [
          'https://www.sana.sy/eng/feed/',
      ],
      'Iraq': [
          'https://www.iraq-business news.com/feed/',
      ],
      'Afghanistan': [
          'https://www.pajhwok.com/en/feed',
      ],
      'Uzbekistan': [
          'https://uzreport.news/feed',
      ],
      'Kazakhstan': [
          'https://astanatimes.com/feed/',
      ],
      'Cuba': [
          'https://en.granma.cu/backend',
      ],
      'Dominican Republic': [
          'https://dominicantoday.com/feed/',
      ],
      'Haiti': [
          'https://lenouvelliste.com/feed/', # This URL might need verification/correction
      ],
      'Guatemala': [
          'https://www.prensalibre.com/feed/', # This URL might need verification/correction
      ],
      'Honduras': [
          'https://www.laprensa.hn/csp/mediapool/sites/LaPrensa/rss.xml', # This URL might need verification/correction
      ],
      'El Salvador': [
          'https://www.elsalvador.com/feed/', # This URL might need verification/correction
      ],
      'Nicaragua': [
          'https://www.laprensa.com.ni/feed/', # This URL might need verification/correction
      ],
      'Costa Rica': [
          'https://ticotimes.net/feed/',
      ],
      'Panama': [
          'https://www.panamatoday.com/feed',
      ]
  }


# Step 4: Initialize Text Processing
print("Initializing...")
print("Setting up text processing...")

# Get English stopwords
try:
    stop_words = set(stopwords.words('english'))
except:
    # Fallback stopwords if NLTK fails
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}

print("Text processing ready!\n")

# Define a timeout handler
class TimeoutException(Exception): pass

@contextmanager
def time_limit(seconds):
    # Simple timeout implementation for Windows
    yield


# Step 5: Text preprocessing functions
def preprocess_text(text):
    """Clean and tokenize text"""
    if not text:
        return []
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove URLs and email addresses (keep this improvement)
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)
    text = re.sub(r'\S+@\S+', ' ', text)
    
    # Remove special characters and numbers, keep only letters and spaces
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)
    
    # Tokenize
    try:
        words = word_tokenize(text)
    except:
        # Fallback to simple split if NLTK fails
        words = text.split()
    
    # Remove stopwords and short words
    words = [word for word in words if word not in stop_words and len(word) > 2]
    
    return words

def get_word_frequency(texts):
    """Get word frequency from a list of texts"""
    all_words = []
    for text in texts:
        words = preprocess_text(text)
        all_words.extend(words)
    
    return Counter(all_words)

# Step 6: Collect news articles from RSS feeds
def get_country_news(country_name, rss_urls, min_articles=50, timeout_seconds=60):
      """Fetch news articles from RSS feeds with a timeout"""
      articles = []

      for rss_url in rss_urls:
          try:
            with time_limit(timeout_seconds):
                  feed = feedparser.parse(rss_url)
                  for entry in feed.entries:
                      title = entry.get('title', '')
                      summary = entry.get('summary', entry.get('description', ''))

                      # Combine title and summary
                      text = f"{title} {summary}"

                      if len(text.strip()) > 20:
                        articles.append({'title': title, 'text': text})

                      # Stop if we have enough
                      if len(articles) >= min_articles:
                          break

                  time.sleep(0.5)  # Be polite to servers

          except TimeoutException:
            print(f"  Timed out fetching articles from {rss_url} for {country_name}")
            continue
          except Exception as e:
            print(f"  Error with {rss_url}: {str(e)[:50]}")
            continue

          if len(articles) >= min_articles:
            break

      print(f"  Found {len(articles)} articles for {country_name}")
      return articles

# Step 7: Find locally common but globally rare words
def find_prevalent_word(country_texts, global_word_counts):
    """Find the most prevalent word that is locally common but globally rare"""
    # Get word frequencies for this country
    country_word_counts = get_word_frequency([article['text'] for article in country_texts])
    
    if not country_word_counts:
        return None, 0
    
    # Define filters for unwanted words - ONLY technical artifacts
    technical_artifacts = {
        'http', 'https', 'www', 'com', 'org', 'net', 'html', 'xml', 'rss', 'feed',
        'url', 'link', 'href', 'src', 'alt', 'title', 'meta', 'tag', 'div', 'span',
        'class', 'id', 'style', 'script', 'css', 'js', 'jpg', 'png', 'gif', 'pdf',
        'doc', 'txt', 'zip', 'mp3', 'mp4', 'avi', 'mov', 'wmv', 'flv', 'swf'
    }
    
    # Calculate local vs global prevalence
    word_scores = {}
    for word, local_count in country_word_counts.items():
        # Skip ONLY technical artifacts
        if word in technical_artifacts:
            continue
        
        global_count = global_word_counts.get(word, 0)
        
        # Calculate local prevalence (frequency in this country)
        local_prevalence = local_count / len(country_texts)
        
        # Calculate global prevalence (frequency across all countries)
        total_global_words = sum(global_word_counts.values())
        global_prevalence = global_count / total_global_words if total_global_words > 0 else 0
        
        # ORIGINAL scoring algorithm - simple and effective
        if global_prevalence > 0:
            score = local_prevalence / global_prevalence
        else:
            # If word doesn't appear globally, use local prevalence
            score = local_prevalence * 100
        
        word_scores[word] = score
    
    # Return the word with highest score
    if word_scores:
        best_word = max(word_scores, key=word_scores.get)
        return best_word, word_scores[best_word]
    
    return None, 0

# Step 8: Main processing loop
print("Collecting and analyzing news articles from RSS feeds...\n")
results = []
all_articles = []  # Store all articles for global word counting

# Create a list of countries to process, starting with Canada
countries_to_process = list(NEWS_SOURCES.keys())
if 'Canada' in countries_to_process:
    countries_to_process.remove('Canada')
countries_to_process.insert(0, 'Canada')

# Apply MAX_COUNTRIES limit if set
if MAX_COUNTRIES is not None:
    countries_to_process = countries_to_process[:MAX_COUNTRIES]

# First pass: collect all articles for global word counting
print("First pass: Collecting articles for global word analysis...")
for country_name in countries_to_process:
    rss_urls = NEWS_SOURCES[country_name]
    print(f"Collecting articles from {country_name}...")

    try:
        with time_limit(60):
            articles = get_country_news(country_name, rss_urls, min_articles=50, timeout_seconds=15)
    except TimeoutException:
        print(f"  Processing for {country_name} timed out after 1 minute. Skipping.")
        continue

    if len(articles) >= 10:
        all_articles.extend(articles)
        print(f"  Collected {len(articles)} articles")
    else:
        print(f"  Not enough articles, skipping...")

# Calculate global word frequencies
print(f"\nCalculating global word frequencies from {len(all_articles)} articles...")
global_word_counts = get_word_frequency([article['text'] for article in all_articles])
print(f"  Found {len(global_word_counts)} unique words globally")

# Second pass: analyze each country for prevalent words
print("\nSecond pass: Finding prevalent words for each country...")
for country_name in countries_to_process:
    rss_urls = NEWS_SOURCES[country_name]
    print(f"Analyzing {country_name}...")

    try:
        with time_limit(60):
            articles = get_country_news(country_name, rss_urls, min_articles=50, timeout_seconds=15)
    except TimeoutException:
        print(f"  Processing for {country_name} timed out after 1 minute. Skipping.")
        continue

    if len(articles) < 10:
        print(f"  Not enough articles, skipping...\n")
        continue

    # Find the most prevalent word for this country
    prevalent_word, score = find_prevalent_word(articles, global_word_counts)
    
    if prevalent_word:
        # Get additional context about the word
        country_word_counts = get_word_frequency([article['text'] for article in articles])
        word_frequency = country_word_counts.get(prevalent_word, 0)
        word_percentage = (word_frequency / len(articles)) * 100

        results.append({
            'country_name': country_name,
            'week': datetime.now().strftime('%Y-W%U'),
            'prevalent_word': prevalent_word,
            'word_frequency': word_frequency,
            'word_percentage': round(word_percentage, 2),
            'prevalence_score': round(score, 2),
            'num_articles': len(articles)
        })
        
        print(f"  Most prevalent word: '{prevalent_word}' (appears in {word_percentage:.1f}% of articles, score: {score:.2f})\n")
    else:
        print(f"  No prevalent word found\n")

# Step 9: Create DataFrame
df = pd.DataFrame(results)

# Check if we have any results
if len(df) == 0:
    print("\n" + "="*80)
    print("NO RESULTS - No articles found for any country")
    print("="*80)
    print("\nPossible issues:")
    print("1. RSS feeds might be temporarily down")
    print("2. Internet connection issues")
    print("3. RSS feed URLs might have changed")
else:
    # Sort by prevalence score
    df = df.sort_values('prevalence_score', ascending=False)

    print("\n" + "="*80)
    print("PREVALENT WORD ANALYSIS RESULTS")
    print("="*80)
    print(df[['country_name', 'prevalent_word', 'word_percentage', 'prevalence_score', 'num_articles']].to_string(index=False))
    print("="*80)

# Step 10: Save to CSV
output_file = f'prevalent_words_rss_{datetime.now().strftime("%Y%m%d")}.csv'
df.to_csv(output_file, index=False)
print(f"\nResults saved to: {output_file}")

# Step 11: Visualization
import matplotlib.pyplot as plt
import numpy as np

# Only create visualizations if we have data
if len(df) > 0:
    # Bar chart for prevalent words
    fig, ax = plt.subplots(figsize=(14, 8))

    countries = df['country_name']
    words = df['prevalent_word']
    scores = df['prevalence_score']
    y_pos = np.arange(len(countries))

    bars = ax.barh(y_pos, scores, color='skyblue')
    ax.set_xlabel('Prevalence Score', fontsize=12)
    ax.set_title('Most Prevalent Words by Country', fontsize=14, fontweight='bold')
    ax.set_yticks(y_pos)
    ax.set_yticklabels([f"{country}\n({word})" for country, word in zip(countries, words)])
    ax.grid(axis='x', alpha=0.3)

    # Add value labels on bars
    for i, (bar, score) in enumerate(zip(bars, scores)):
        ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, 
               f'{score:.1f}', ha='left', va='center', fontsize=10)

    plt.tight_layout()
    # plt.show()  # Disabled - we don't need the bar plot anymore

    print("\nDone! Prevalent word analysis complete.")
else:
    print("\nNo data to visualize.")

print("\nSummary:")
if len(df) > 0:
    print(f"   Countries analyzed: {len(df)}")
    print(f"   Total articles processed: {df['num_articles'].sum()}")
    print(f"   Most prevalent word globally: {df.loc[df['prevalence_score'].idxmax(), 'prevalent_word']} "
          f"in {df.loc[df['prevalence_score'].idxmax(), 'country_name']}")
else:
    print("   No results to summarize.")